# -*- coding: utf-8 -*-
"""main.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1CL2aTRegC0pcLAs1LwsXpPIY5LQWujK9
"""

'''
Pyspark Sub Module 

'''
from pyspark.sql.types import *
from pyspark.sql import SparkSession
from pyspark.context import SparkContext
from pyspark.sql.types import StructType,StructField, StringType, IntegerType
import pyspark.sql.functions as F


# def main(df1,df2):


#     sdf, ftype = execute_bronze(spark,dataframe,delimiter=',')
#     src_sdf = execute_silver(sdf,ftype)
#     tgt_sdf = execute_silver(sdf,ftype)
#     a,b,c = parquet_build(src_sdf,tgt_sdf)
#     execute_gold(a,b,c)

def main(df1,df2):
    df1=casting(df1)
    df2=casting(df2)

def casting(df):
    df = df.withColumn('Emp_ID', when(length(trim(col('Emp_ID'))) == 0, trim('Emp_ID')).otherwise(0))\
                       .withColumn('Date_of_Joining', when(length('Date_of_Joining')==0,'00/00/000 00:00:00')\
                                  .otherwise(date_format(Date_Convert(col('DATE')), 'MM-dd-yyyy')))\ 
                       .withColumn("Age_in_Yrs", coalesce(col("Age_in_Yrs"), lit("0.00"))\
                       .withColumn("Age_in_Company_Yrs", coalesce(col("Age_in_Company_Yrs"), lit("0.00"))\
                       .withColumn("Salary", coalesce(col("Salary"), lit("0.00"))\
                       .withColumn('Zip', F.lpad(df['Zip'], 5, '0'))
    return df

def parquet_build(old,new):

  key = "Emp_ID"
  new_emp = old.join(new, Seq(key), "right_outer")\
              .filter($"old.value" isNull)\
              .drop($"old.value")

  mod_emp = old.join(new, Seq(key))\ #inner
              .filter($"new.value" isNull)\
              .drop($"new.value")

  del_emp = old.join(new, Seq(key), "left_outer")\  # needs to be checked 
              .filter($"new.value" isNull)\
              .drop($"new.value")

              # is collect() needed before parquet ???
  new_emp = new_emp.withColumn("flag", lit('I'))
  mod_emp = mod_emp.withColumn("flag", lit('C'))
  del_emp = del_emp.withColumn("flag", lit('D'))

  return new_emp, mod_emp, del_emp

def execute_bronze(spark,source_data,delimiter=','):

#force data type

    if source_data.split('.')[-1] == 'csv':
      ftype='csv'
      data_sdf = spark.read.csv(soruce_data).delimiter(delimiter)
    
    if source_data.split('.')[-1] == 'xlsx':
      data_sdf = spark.read.format("com.crealytics.spark.excel")\
        .option("dataAddress", "file_name!A1")\
        .option("header", "true")\
        .option("treatEmptyValuesAsNulls", "false")\
        .option("inferSchema", "true")\
        .option("timestampFormat", "MM-dd-yyyy HH:mm:ss")\
        .option("maxRowsInMemory", 20)\
        .load(source_data)

    return data_sdf, ftype

dyf_applyMapping = ApplyMapping.apply( frame = dyf_orders, mappings = [
("Emp ID","String","Emp_ID","Long"), 
("Name Prefix","String","Name_Prefix","Long"),
("First Name","String","First_Name","String"),
("Middle Initial","String","Middle_Initial","Long"), 
("zipcode","String","zip","Long") 
  ])

def execute_silver(data_sdf, ftype):
    # Declarations
    Date_Convert =  udf (lambda x: datetime.strptime(x, '%Y%m%d'), DateType()) 

    # Match to DB column names 
    #glue code
    df1 = RenameField.apply[
                            (frame = df, old_name = "col1", new_name = "COL1"),
                            (frame = df, old_name = "col2", new_name = "COL2"),
                            (frame = df, old_name = "col2", new_name = "COL2"),
                            (frame = df, old_name = "col2", new_name = "COL2"),
                            (frame = df, old_name = "col2", new_name = "COL2"),
                            (frame = df, old_name = "col2", new_name = "COL2"),
                            (frame = df, old_name = "col2", new_name = "COL2"),
                            (frame = df, old_name = "col2", new_name = "COL2"),
                            (frame = df, old_name = "col2", new_name = "COL2"),
                            (frame = df, old_name = "col2", new_name = "COL2"),
                            (frame = df, old_name = "col2", new_name = "COL2"),
                            (frame = df, old_name = "col2", new_name = "COL2"),
                            (frame = df, old_name = "col2", new_name = "COL2"),
                            (frame = df, old_name = "col2", new_name = "COL2"),
                            (frame = df, old_name = "col2", new_name = "COL2"),
                            ]
    #glue code
    # applymapping1 = ApplyMapping.apply(frame = datasource0, mappings = [("policyid", "long", "policyid", "long"), 
    sdf = sdf.withColumnRenamed("Emp ID"                ,"Emp_ID")\
             .withColumnRenamed("Name Prefix"           ,"Name_Prefix")\
             .withColumnRenamed("First Name"            ,"First_Name")\
             .withColumnRenamed("Middle Initial"        ,"Middle_Initial")\
             .withColumnRenamed("Last Name"             ,"Last_Name")\
             .withColumnRenamed("Gender"                ,"Gender")\
             .withColumnRenamed("Age in Yrs."           ,"Age_in_Yrs")\
             .withColumnRenamed("Date of Joining"       ,"Date_of_Joining")\
             .withColumnRenamed("Age in Company (Years)","Age_in_Company_Yrs")\
             .withColumnRenamed("Salary"                ,"Salary")\
             .withColumnRenamed("Place Name"            ,"Place_Name")\
             .withColumnRenamed("County"                ,"County")\
             .withColumnRenamed("City"                  ,"City")\
             .withColumnRenamed("State"                 ,"State")\
             .withColumnRenamed("Zip"                   ,"Zip")
    
    # glue code : df = df.resolveChoice(specs = [('Emp_ID','cast:Long')])
    df = df.resolveChoice(specs = [
                                   ('Emp_ID','cast:long'),
                                   ('Name_Prefix','cast:string'),
                                   ('First_Name','cast:string'),
                                   ('Middle_Initial','cast:string'),
                                   ('Last_Name','cast:string'),
                                   ('Gender','cast:string'),
                                   ('Age_in_Yrs','cast:double'),
                                   ('Date_of_Joining','cast:date'),
                                   ('Age_in_Company_Yrs','cast:double'),
                                   ('Salary','cast:double'),
                                   ('Place_Name','cast:string'),
                                   ('County','cast:string'),
                                   ('City','cast:string'),
                                   ('State','cast:string'),
                                   ('Zip','cast:string'),
                                   ])
    # glue code end
    # defining custom schema & force datatype
    sdf_schema = StructType([\
        StructField("Emp_ID"            ,LongType()  ,True),\
        StructField("Name_Prefix"       ,StringType(),True),\
        StructField("First_Name"        ,StringType(),True),\
        StructField("Middle_Initial"    ,StringType(),True),\
        StructField("Last_Name"         ,StringType(),True),\
        StructField("Gender"            ,StringType(),True),\
        StructField("Age_in_Yrs"        ,DoubleType(),True),\
        StructField("Date_of_Joining"   ,DateType()  ,True),\
        StructField("Age_in_Company_Yrs",DoubleType(),True),\
        StructField("Salary"            ,DoubleType(),True),\
        StructField("Place_Name"        ,StringType(),True),\
        StructField("County"            ,StringType(),True),\
        StructField("City"              ,StringType(),True),\
        StructField("State"             ,StringType(),True),\
        StructField("Zip"               ,StringType(),True),\
    ])



    # Value Conversion & Handling Nulls   ##.cast(BooleanType)
    data_sdf = data_sdf.withColumn('Emp_ID', when(length(trim(col('Emp_ID'))) == 0, trim('Emp_ID')).otherwise(0))\
                       .withColumn('Date_of_Joining', when(length('Date_of_Joining')==0,'00/00/000 00:00:00').otherwise(date_format(Date_Convert(col('DATE')), 'MM-dd-yyyy'))).\ 
                       .withColumn("Age_in_Yrs", coalesce(col("Age_in_Yrs"), lit("0.00"))
                       .withColumn("Age_in_Company_Yrs", coalesce(col("Age_in_Company_Yrs"), lit("0.00"))
                       .withColumn("Salary", coalesce(col("Salary"), lit("0.00"))
                       .withColumn('Zip', F.lpad(data_sdf['Zip'], 5, '0'))

def execute_gold(*sdf):
    for table in sdf:
      table.repartition(1).write.mode('append').parquet('sample_data_empl_dummy')

def main():
    init()