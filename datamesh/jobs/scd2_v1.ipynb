{
 "cells": [
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/susi/Project/datamesh/jobs\r\n"
     ]
    }
   ],
   "source": [
    "# !export PYSPARK_SUBMIT_ARGS = \"--master spark://192.168.2.40:7077\"0\n",
    "# !echo $SPARK_HOME\n",
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
=======
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
>>>>>>> 3080610c4e1eebe5a26bb3371dc2fdf7895090f3
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "Java gateway process exited before sending its port number",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-b8584f26c7aa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mspark\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuilder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappName\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"scd2_demo\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetOrCreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;31m# v_s3_path = \"s3://mybucket/dim_customer_scd\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/susi/Downloads/spark-3.1.2-bin-hadoop3.2/python/pyspark/sql/session.py\u001b[0m in \u001b[0;36mgetOrCreate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    226\u001b[0m                             \u001b[0msparkConf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m                         \u001b[0;31m# This SparkContext may be an existing one.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 228\u001b[0;31m                         \u001b[0msc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetOrCreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msparkConf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    229\u001b[0m                     \u001b[0;31m# Do not update `SparkConf` for existing `SparkContext`, as it's shared\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m                     \u001b[0;31m# by all sessions.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/susi/Downloads/spark-3.1.2-bin-hadoop3.2/python/pyspark/context.py\u001b[0m in \u001b[0;36mgetOrCreate\u001b[0;34m(cls, conf)\u001b[0m\n\u001b[1;32m    382\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 384\u001b[0;31m                 \u001b[0mSparkContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconf\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mSparkConf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    385\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/susi/Downloads/spark-3.1.2-bin-hadoop3.2/python/pyspark/context.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls)\u001b[0m\n\u001b[1;32m    142\u001b[0m                 \" is not allowed as it is a security risk.\")\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m         \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgateway\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    145\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m             self._do_init(master, appName, sparkHome, pyFiles, environment, batchSize, serializer,\n",
      "\u001b[0;32m/home/susi/Downloads/spark-3.1.2-bin-hadoop3.2/python/pyspark/context.py\u001b[0m in \u001b[0;36m_ensure_initialized\u001b[0;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[1;32m    329\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gateway\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 331\u001b[0;31m                 \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gateway\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgateway\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mlaunch_gateway\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    332\u001b[0m                 \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gateway\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjvm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/susi/Downloads/spark-3.1.2-bin-hadoop3.2/python/pyspark/java_gateway.py\u001b[0m in \u001b[0;36mlaunch_gateway\u001b[0;34m(conf, popen_kwargs)\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconn_info_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Java gateway process exited before sending its port number\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconn_info_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mException\u001b[0m: Java gateway process exited before sending its port number"
     ]
    }
   ],
   "source": [
    "\n",
    "spark = SparkSession.builder.appName(\"scd2_demo\").getOrCreate()\n",
    "# v_s3_path = \"s3://mybucket/dim_customer_scd\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hd_current_scd2 = \"\"\"\n",
    " SELECT   BIGINT(1) AS customer_dim_key,\n",
    "          STRING('John') AS first_name,\n",
    "          STRING('Smith') AS last_name,\n",
    "          STRING('G') AS middle_initial,\n",
    "          STRING('123 Main Street') AS address,\n",
    "          STRING('Springville') AS city,\n",
    "          STRING('VT') AS state,\n",
    "          STRING('01234-5678') AS zip_code,\n",
    "          BIGINT(289374) AS customer_number,\n",
    "          DATE('2014-01-01') AS eff_start_date,\n",
    "          DATE('9999-12-31') AS eff_end_date,\n",
    "          BOOLEAN(1) AS is_current\n",
    " UNION\n",
    " SELECT   BIGINT(2) AS customer_dim_key,\n",
    "          STRING('Susan') AS first_name,\n",
    "          STRING('Jones') AS last_name,\n",
    "          STRING('L') AS middle_initial,\n",
    "          STRING('987 Central Avenue') AS address,\n",
    "          STRING('Central City') AS city,\n",
    "          STRING('MO') AS state,\n",
    "          STRING('49257-2657') AS zip_code,\n",
    "          BIGINT(862447) AS customer_number,\n",
    "          DATE('2015-03-23') AS eff_start_date,\n",
    "          DATE('2018-11-17') AS eff_end_date,\n",
    "          BOOLEAN(0) AS is_current\n",
    " UNION\n",
    " SELECT   BIGINT(3) AS customer_dim_key,\n",
    "          STRING('Susan') AS first_name,\n",
    "          STRING('Harris') AS last_name,\n",
    "          STRING('L') AS middle_initial,\n",
    "          STRING('987 Central Avenue') AS address,\n",
    "          STRING('Central City') AS city,\n",
    "          STRING('MO') AS state,\n",
    "          STRING('49257-2657') AS zip_code,\n",
    "          BIGINT(862447) AS customer_number,\n",
    "          DATE('2018-11-18') AS eff_start_date,\n",
    "          DATE('9999-12-31') AS eff_end_date,\n",
    "          BOOLEAN(1) AS is_current\n",
    " UNION\n",
    " SELECT   BIGINT(4) AS customer_dim_key,\n",
    "          STRING('William') AS first_name,\n",
    "          STRING('Chase') AS last_name,\n",
    "          STRING('X') AS middle_initial,\n",
    "          STRING('57895 Sharp Way') AS address,\n",
    "          STRING('Oldtown') AS city,\n",
    "          STRING('CA') AS state,\n",
    "          STRING('98554-1285') AS zip_code,\n",
    "          BIGINT(31568) AS customer_number,\n",
    "          DATE('2018-12-07') AS eff_start_date,\n",
    "          DATE('9999-12-31') AS eff_end_date,\n",
    "          BOOLEAN(1) AS is_current\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_current_scd2 = spark.sql(hd_current_scd2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_current_scd2.createOrReplaceTempView(\"current_scd2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hd_customer_data = \"\"\"\n",
    " SELECT   BIGINT(289374) AS customer_number,\n",
    "          STRING('John') AS first_name,\n",
    "          STRING('Smith') AS last_name,\n",
    "          STRING('G') AS middle_initial,\n",
    "          STRING('456 Derry Court') AS address,\n",
    "          STRING('Springville') AS city,\n",
    "          STRING('VT') AS state,\n",
    "          STRING('01234-5678') AS zip_code\n",
    " UNION\n",
    " SELECT   BIGINT(932574) AS customer_number,\n",
    "          STRING('Lisa') AS first_name,\n",
    "          STRING('Cohen') AS last_name,\n",
    "          STRING('S') AS middle_initial,\n",
    "          STRING('69846 Mason Road') AS address,\n",
    "          STRING('Atlanta') AS city,\n",
    "          STRING('GA') AS state,\n",
    "          STRING('26584-3591') AS zip_code\n",
    " UNION\n",
    " SELECT   BIGINT(862447) AS customer_number,\n",
    "          STRING('Susan') AS first_name,\n",
    "          STRING('Harris') AS last_name,\n",
    "          STRING('L') AS middle_initial,\n",
    "          STRING('987 Central Avenue') AS address,\n",
    "          STRING('Central City') AS city,\n",
    "          STRING('MO') AS state,\n",
    "          STRING('49257-2657') AS zip_code\n",
    " UNION\n",
    " SELECT   BIGINT(31568) AS customer_number,\n",
    "          STRING('William') AS first_name,\n",
    "          STRING('Chase') AS last_name,\n",
    "          STRING('X') AS middle_initial,\n",
    "          STRING('57895 Sharp Way') AS address,\n",
    "          STRING('Oldtown') AS city,\n",
    "          STRING('CA') AS state,\n",
    "          STRING('98554-1285') AS zip_code\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_customer_data= spark.sql(hd_customer_data)\n",
    "df_customer_data.createOrReplaceTempView(\"customer_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_customer_data.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hd_new_curr_recs = \"\"\"\n",
    " SELECT   t.customer_dim_key,\n",
    "          s.customer_number,\n",
    "          s.first_name,\n",
    "          s.last_name,\n",
    "          s.middle_initial,\n",
    "          s.address,\n",
    "          s.city,\n",
    "          s.state,\n",
    "          s.zip_code,\n",
    "          DATE(FROM_UTC_TIMESTAMP(CURRENT_TIMESTAMP, 'PST'))\n",
    "              AS eff_start_date,\n",
    "          DATE('9999-12-31') AS eff_end_date,\n",
    "          BOOLEAN(1) AS is_current\n",
    " FROM     customer_data s\n",
    "          INNER JOIN current_scd2 t\n",
    "              ON t.customer_number = s.customer_number\n",
    "              AND t.is_current = True\n",
    " WHERE    NVL(s.first_name, '') <> NVL(t.first_name, '')\n",
    "          OR NVL(s.last_name, '') <> NVL(t.last_name, '')\n",
    "          OR NVL(s.middle_initial, '') <> NVL(t.middle_initial, '')\n",
    "          OR NVL(s.address, '') <> NVL(t.address, '')\n",
    "          OR NVL(s.city, '') <> NVL(t.city, '')\n",
    "          OR NVL(s.state, '') <> NVL(t.state, '')\n",
    "          OR NVL(s.zip_code, '') <> NVL(t.zip_code, '')\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_new_curr_recs = spark.sql(hd_new_curr_recs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_new_curr_recs.createOrReplaceTempView(\"new_curr_recs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new_curr_recs.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_modfied_keys = df_new_curr_recs.select(\"customer_dim_key\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_modfied_keys.createOrReplaceTempView(\"modfied_keys\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hd_new_hist_recs = \"\"\"\n",
    " SELECT   t.customer_dim_key,\n",
    "          t.customer_number,\n",
    "          t.first_name,\n",
    "          t.last_name,\n",
    "          t.middle_initial,\n",
    "          t.address,\n",
    "          t.city,\n",
    "          t.state,\n",
    "          t.zip_code,\n",
    "          t.eff_start_date,\n",
    "          DATE_SUB(\n",
    "              DATE(FROM_UTC_TIMESTAMP(CURRENT_TIMESTAMP, 'PST')), 1\n",
    "          ) AS eff_end_date,\n",
    "          BOOLEAN(0) AS is_current\n",
    " FROM     current_scd2 t\n",
    "          INNER JOIN modfied_keys k\n",
    "              ON k.customer_dim_key = t.customer_dim_key\n",
    " WHERE    t.is_current = True\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_new_hist_recs = spark.sql(hd_new_hist_recs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_new_hist_recs.createOrReplaceTempView(\"new_hist_recs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new_hist_recs.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hd_unaffected_recs = \"\"\"\n",
    " SELECT   s.customer_dim_key,\n",
    "          s.customer_number,\n",
    "          s.first_name,\n",
    "          s.last_name,\n",
    "          s.middle_initial,\n",
    "          s.address,\n",
    "          s.city,\n",
    "          s.state,\n",
    "          s.zip_code,\n",
    "          s.eff_start_date,\n",
    "          s.eff_end_date,\n",
    "          s.is_current\n",
    " FROM     current_scd2 s\n",
    "          LEFT OUTER JOIN modfied_keys k\n",
    "              ON k.customer_dim_key = s.customer_dim_key\n",
    " WHERE    k.customer_dim_key IS NULL\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_unaffected_recs = spark.sql(hd_unaffected_recs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_unaffected_recs.createOrReplaceTempView(\"unaffected_recs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_unaffected_recs.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hd_new_cust = \"\"\"\n",
    " SELECT   s.customer_number,\n",
    "          s.first_name,\n",
    "          s.last_name,\n",
    "          s.middle_initial,\n",
    "          s.address,\n",
    "          s.city,\n",
    "          s.state,\n",
    "          s.zip_code,\n",
    "          DATE(FROM_UTC_TIMESTAMP(CURRENT_TIMESTAMP, 'PST')) \n",
    "              AS eff_start_date,\n",
    "          DATE('9999-12-31') AS eff_end_date,\n",
    "          BOOLEAN(1) AS is_current\n",
    " FROM     customer_data s\n",
    "          LEFT OUTER JOIN current_scd2 t\n",
    "              ON t.customer_number = s.customer_number\n",
    " WHERE    t.customer_number IS NULL\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_new_cust = spark.sql(hd_new_cust)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_new_cust.createOrReplaceTempView(\"new_cust\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new_cust.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "v_max_key = spark.sql(\n",
    "    \"SELECT STRING(MAX(customer_dim_key)) FROM current_scd2\"\n",
    ").collect()[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v_max_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hd_new_scd2 = \"\"\"\n",
    " WITH a_cte\n",
    " AS   (\n",
    "        SELECT     x.first_name, x.last_name,\n",
    "                   x.middle_initial, x.address,\n",
    "                   x.city, x.state, x.zip_code,\n",
    "                   x.customer_number, x.eff_start_date,\n",
    "                   x.eff_end_date, x.is_current\n",
    "        FROM       new_cust x\n",
    "        UNION ALL\n",
    "        SELECT     y.first_name, y.last_name,\n",
    "                   y.middle_initial, y.address,\n",
    "                   y.city, y.state, y.zip_code,\n",
    "                   y.customer_number, y.eff_start_date,\n",
    "                   y.eff_end_date, y.is_current\n",
    "        FROM       new_curr_recs y\n",
    "      )\n",
    "  ,   b_cte\n",
    "  AS  (\n",
    "        SELECT  ROW_NUMBER() OVER(ORDER BY a.eff_start_date)\n",
    "                    + BIGINT('{v_max_key}') AS customer_dim_key,\n",
    "                a.first_name, a.last_name,\n",
    "                a.middle_initial, a.address,\n",
    "                a.city, a.state, a.zip_code,\n",
    "                a.customer_number, a.eff_start_date,\n",
    "                a.eff_end_date, a.is_current\n",
    "        FROM    a_cte a\n",
    "      )\n",
    "  SELECT  customer_dim_key, first_name, last_name,\n",
    "          middle_initial, address,\n",
    "          city, state, zip_code,\n",
    "          customer_number, eff_start_date,\n",
    "          eff_end_date, is_current\n",
    "  FROM    b_cte\n",
    "  UNION ALL\n",
    "  SELECT  customer_dim_key, first_name,  last_name,\n",
    "          middle_initial, address,\n",
    "          city, state, zip_code,\n",
    "          customer_number, eff_start_date,\n",
    "          eff_end_date, is_current\n",
    "  FROM    unaffected_recs\n",
    "  UNION ALL\n",
    "  SELECT  customer_dim_key, first_name,  last_name,\n",
    "          middle_initial, address,\n",
    "          city, state, zip_code,\n",
    "          customer_number, eff_start_date,\n",
    "          eff_end_date, is_current\n",
    "  FROM    new_hist_recs\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_new_scd2 = spark.sql(hd_new_scd2.replace(\"{v_max_key}\", v_max_key))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_new_scd2.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_customer_data.coalesce(1).write.csv(\"/home/susi/data/current_scd2.csv\")\n",
    "df_new_curr_recs.coalesce(1).write.csv(\"/home/susi/data/df_new_curr_recs.csv\")\n",
    "df_new_hist_recs.coalesce(1).write.csv(\"/home/susi/data/df_new_hist_recs.csv\")\n",
    "df_unaffected_recs.coalesce(1).write.csv(\"/home/susi/data/df_unaffected_recs.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+----------+---------+--------------+------------------+------------+-----+----------+\n",
      "|customer_number|first_name|last_name|middle_initial|           address|        city|state|  zip_code|\n",
      "+---------------+----------+---------+--------------+------------------+------------+-----+----------+\n",
      "|         932574|      Lisa|    Cohen|             S|  69846 Mason Road|     Atlanta|   GA|26584-3591|\n",
      "|          31568|   William|    Chase|             X|   57895 Sharp Way|     Oldtown|   CA|98554-1285|\n",
      "|         862447|     Susan|   Harris|             L|987 Central Avenue|Central City|   MO|49257-2657|\n",
      "|         289374|      John|    Smith|             G|   456 Derry Court| Springville|   VT|01234-5678|\n",
      "+---------------+----------+---------+--------------+------------------+------------+-----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_customer_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+---------------+----------+---------+--------------+---------------+-----------+-----+----------+--------------+------------+----------+\n",
      "|customer_dim_key|customer_number|first_name|last_name|middle_initial|        address|       city|state|  zip_code|eff_start_date|eff_end_date|is_current|\n",
      "+----------------+---------------+----------+---------+--------------+---------------+-----------+-----+----------+--------------+------------+----------+\n",
      "|               1|         289374|      John|    Smith|             G|456 Derry Court|Springville|   VT|01234-5678|    2021-07-09|  9999-12-31|      true|\n",
      "+----------------+---------------+----------+---------+--------------+---------------+-----------+-----+----------+--------------+------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_new_curr_recs.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+---------------+----------+---------+--------------+---------------+-----------+-----+----------+--------------+------------+----------+\n",
      "|customer_dim_key|customer_number|first_name|last_name|middle_initial|        address|       city|state|  zip_code|eff_start_date|eff_end_date|is_current|\n",
      "+----------------+---------------+----------+---------+--------------+---------------+-----------+-----+----------+--------------+------------+----------+\n",
      "|               1|         289374|      John|    Smith|             G|123 Main Street|Springville|   VT|01234-5678|    2014-01-01|  2021-07-08|     false|\n",
      "+----------------+---------------+----------+---------+--------------+---------------+-----------+-----+----------+--------------+------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_new_hist_recs.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+---------------+----------+---------+--------------+------------------+------------+-----+----------+--------------+------------+----------+\n",
      "|customer_dim_key|customer_number|first_name|last_name|middle_initial|           address|        city|state|  zip_code|eff_start_date|eff_end_date|is_current|\n",
      "+----------------+---------------+----------+---------+--------------+------------------+------------+-----+----------+--------------+------------+----------+\n",
      "|               4|          31568|   William|    Chase|             X|   57895 Sharp Way|     Oldtown|   CA|98554-1285|    2018-12-07|  9999-12-31|      true|\n",
      "|               2|         862447|     Susan|    Jones|             L|987 Central Avenue|Central City|   MO|49257-2657|    2015-03-23|  2018-11-17|     false|\n",
      "|               3|         862447|     Susan|   Harris|             L|987 Central Avenue|Central City|   MO|49257-2657|    2018-11-18|  9999-12-31|      true|\n",
      "+----------------+---------------+----------+---------+--------------+------------------+------------+-----+----------+--------------+------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_unaffected_recs.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
