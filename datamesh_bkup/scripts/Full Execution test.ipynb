{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:200% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:200% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "# from pyspark.sql.functions import udf,DataType,when,length,trim,col,lit\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType,StructField,DateType,LongType,DoubleType,StringType,IntegerType,TimestampType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "Java gateway process exited before sending its port number",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-d8dc703b64cb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mspark\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBuilder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaster\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"local\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m           \u001b[0;34m.\u001b[0m\u001b[0mappName\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"tmpdriver\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m           \u001b[0;34m.\u001b[0m\u001b[0mgetOrCreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mfile1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/home/susi/Project/datamesh/data_bkup/data/smaple_data_empl_dummy_1.csv\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/susi/Downloads/spark-3.1.2-bin-hadoop3.2/python/pyspark/sql/session.py\u001b[0m in \u001b[0;36mgetOrCreate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    226\u001b[0m                             \u001b[0msparkConf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m                         \u001b[0;31m# This SparkContext may be an existing one.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 228\u001b[0;31m                         \u001b[0msc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetOrCreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msparkConf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    229\u001b[0m                     \u001b[0;31m# Do not update `SparkConf` for existing `SparkContext`, as it's shared\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m                     \u001b[0;31m# by all sessions.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/susi/Downloads/spark-3.1.2-bin-hadoop3.2/python/pyspark/context.py\u001b[0m in \u001b[0;36mgetOrCreate\u001b[0;34m(cls, conf)\u001b[0m\n\u001b[1;32m    382\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 384\u001b[0;31m                 \u001b[0mSparkContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconf\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mSparkConf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    385\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/susi/Downloads/spark-3.1.2-bin-hadoop3.2/python/pyspark/context.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls)\u001b[0m\n\u001b[1;32m    142\u001b[0m                 \" is not allowed as it is a security risk.\")\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m         \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgateway\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    145\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m             self._do_init(master, appName, sparkHome, pyFiles, environment, batchSize, serializer,\n",
      "\u001b[0;32m/home/susi/Downloads/spark-3.1.2-bin-hadoop3.2/python/pyspark/context.py\u001b[0m in \u001b[0;36m_ensure_initialized\u001b[0;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[1;32m    329\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gateway\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 331\u001b[0;31m                 \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gateway\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgateway\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mlaunch_gateway\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    332\u001b[0m                 \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gateway\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjvm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/susi/Downloads/spark-3.1.2-bin-hadoop3.2/python/pyspark/java_gateway.py\u001b[0m in \u001b[0;36mlaunch_gateway\u001b[0;34m(conf, popen_kwargs)\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconn_info_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Java gateway process exited before sending its port number\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconn_info_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mException\u001b[0m: Java gateway process exited before sending its port number"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.Builder().master(\"local\") \\\n",
    "          .appName(\"tmpdriver\") \\\n",
    "          .getOrCreate()\n",
    "\n",
    "file1 = \"/home/susi/Project/datamesh/data_bkup/data/smaple_data_empl_dummy_1.csv\"\n",
    "file2 = \"/home/susi/Project/datamesh/data_bkup/data/smaple_data_empl_dummy_2.csv\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = spark.read.options(delimiter=',',header=\"true\").csv(file1)\n",
    "df2 = spark.read.options(delimiter=',',header=\"true\").csv(file2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1_2 = df1. \\\n",
    " withColumnRenamed(\"Emp ID\"                ,\"Emp_ID\")\\\n",
    ".withColumnRenamed(\"Name Prefix\"           ,\"Name_Prefix\")\\\n",
    ".withColumnRenamed(\"First Name\"            ,\"First_Name\")\\\n",
    ".withColumnRenamed(\"Middle Initial\"        ,\"Middle_Initial\")\\\n",
    ".withColumnRenamed(\"Last Name\"             ,\"Last_Name\")\\\n",
    ".withColumnRenamed(\"Gender\"                ,\"Gender\")\\\n",
    ".withColumnRenamed(\"Age in Yrs.\"           ,\"Age_in_Yrs\")\\\n",
    ".withColumnRenamed(\"Date of Joining\"       ,\"Date_of_Joining\")\\\n",
    ".withColumnRenamed(\"Age in Company (Years)\",\"Age_in_Company_Yrs\")\\\n",
    ".withColumnRenamed(\"Salary\"                ,\"Salary\")\\\n",
    ".withColumnRenamed(\"Place Name\"            ,\"Place_Name\")\\\n",
    ".withColumnRenamed(\"County\"                ,\"County\")\\\n",
    ".withColumnRenamed(\"City\"                  ,\"City\")\\\n",
    ".withColumnRenamed(\"State\"                 ,\"State\")\\\n",
    ".withColumnRenamed(\"Zip\"                   ,\"Zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1_2.printSchema()\n",
    "# df1_2.filln"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sdf_schema = StructType([\\\n",
    "#         StructField(\"Emp_ID\"            ,LongType()  ,True),\\\n",
    "#         StructField(\"Name_Prefix\"       ,StringType(),True),\\\n",
    "#         StructField(\"First_Name\"        ,StringType(),True),\\\n",
    "#         StructField(\"Middle_Initial\"    ,StringType(),True),\\\n",
    "#         StructField(\"Last_Name\"         ,StringType(),True),\\\n",
    "#         StructField(\"Gender\"            ,StringType(),True),\\\n",
    "#         StructField(\"Age_in_Yrs\"        ,DoubleType(),True),\\\n",
    "#         StructField(\"Date_of_Joining\"   ,DateType()  ,True),\\\n",
    "#         StructField(\"Age_in_Company_Yrs\",DoubleType(),True),\\\n",
    "#         StructField(\"Salary\"            ,DoubleType(),True),\\\n",
    "#         StructField(\"Place_Name\"        ,StringType(),True),\\\n",
    "#         StructField(\"County\"            ,StringType(),True),\\\n",
    "#         StructField(\"City\"              ,StringType(),True),\\\n",
    "#         StructField(\"State\"             ,StringType(),True),\\\n",
    "#         StructField(\"Zip\"               ,StringType(),True),\\\n",
    "#     ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1_3 = df1_2 \\\n",
    ".withColumn('Emp_ID', when(length(trim(col('Emp_ID'))) == 0 ,None).otherwise(df1_2[\"Emp_ID\"].cast(LongType()))) \\\n",
    ".withColumn('Name_Prefix', when(length(trim(col('Name_Prefix'))) == 0 ,None).otherwise(df1_2[\"Name_Prefix\"].cast(StringType()))) \\\n",
    ".withColumn('First_Name', when(length(trim(col('First_Name'))) == 0 ,None).otherwise(df1_2[\"First_Name\"].cast(StringType()))) \\\n",
    ".withColumn('Middle_Initial', when(length(trim(col('Middle_Initial'))) == 0 ,None).otherwise(df1_2[\"Middle_Initial\"].cast(StringType()))) \\\n",
    ".withColumn('Last_Name', when(length(trim(col('Last_Name'))) == 0 ,None).otherwise(df1_2[\"Last_Name\"].cast(StringType()))) \\\n",
    ".withColumn('Gender', when(length(trim(col('Gender'))) == 0 ,None).otherwise(df1_2[\"Gender\"].cast(StringType()))) \\\n",
    ".withColumn('Age_in_Yrs',      when(length(trim(     col('Age_in_Yrs'))) == 0 , None).otherwise(df1_2[\"Age_in_Yrs\"].cast(DoubleType()))) \\\n",
    ".withColumn('Age_in_Company_Yrs', when(length(trim(col('Age_in_Company_Yrs'))) == 0 ,None).otherwise(df1_2[\"Age_in_Company_Yrs\"].cast(DoubleType())))\\\n",
    ".withColumn('Salary', when(length(trim(col('Salary'))) == 0 ,None).otherwise(df1_2[\"Salary\"].cast(DoubleType())))\\\n",
    ".withColumn('Place_Name', when(length(trim(col('Place_Name'))) == 0 ,None).otherwise(df1_2[\"Place_Name\"].cast(StringType()))) \\\n",
    ".withColumn('County', when(length(trim(col('County'))) == 0 ,None).otherwise(df1_2[\"County\"].cast(StringType()))) \\\n",
    ".withColumn('City', when(length(trim(col('City'))) == 0 ,None).otherwise(df1_2[\"City\"].cast(StringType()))) \\\n",
    ".withColumn('State', when(length(trim(col('State'))) == 0 ,None).otherwise(df1_2[\"State\"].cast(StringType()))) \\\n",
    ".withColumn('Zip', when(length(trim(col('Zip'))) == 0 ,None).otherwise(df1_2[\"Zip\"].cast(StringType())))\n",
    "\n",
    "\n",
    "\n",
    "df1_3=df1_3.fillna(\"00/00/0000 00:00:00\",[\"Date_of_Joining\"]) \n",
    "\n",
    "\n",
    "\n",
    "df1_3.show()  \n",
    "# df1_3.printSchema()\n",
    "\n",
    "# df1_2.to_timestamp()\n",
    "            \n",
    "# .withColumn('Date_of_Joining', when(length(trim(col('Date_of_Joining'))) == 0 ,'missing').otherwise(df1_2[\"Date_of_Joining\"].cast(DateType()))) \\\n",
    "# .withColumn(\"Date_of_Joining\",    when(length(trim(col('Date_of_Joining'))) != 0 ,to_timestamp(\"Date_of_Joining\").cast('date')))\\\n",
    "# .withColumn('Date_of_Joining',    when(length(trim(col('Date_of_Joining'))) == 0 ,'00/00/0000 00:00:00'))\\\n",
    "# .withColumn(\"Date_of_Joining\", when(length(trim(col('Date_of_Joining')))!=0,to_timestamp(\"Date_of_Joining\", \"MM_dd_yyyy\"))).otherwise(lit('00/00/0000 00:00:00'))) \\\n",
    "# .withColumn('Date_of_Joining', when(length(trim(     col('Date_of_Joining'))) == 0 , '00/00/0000 00:00:00').otherwise(to_timestamp(\"Date_of_Joining\", \"MM/DD/yyyy\"))) \\\n",
    "# .withColumn('Date_of_Joining', col('Date_of_Joining').cast('timestamp'))\\\n",
    "\n",
    "#  from_unixtime(unix_timestamp(from_unixtime(unix_timestamp('ts_string', 'MM/dd/yyyy hh:mm:ss a')).cast(TimestampType()), 'MM-dd-yyyy hh:mm:ss')).alias(\"timestamp2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df1_3 = df1_2.withColumn('Emp_ID', when(length(trim(col('Emp_ID'))) == 0),col('Emp_ID'))\n",
    "#                        .withColumn('Name_Prefix', when(length('Name_Prefix')==0,Null))\\\n",
    "#                        .withColumn('First_Name', when(length('First_Name')==0,Null))\\\n",
    "#                        .withColumn('Middle_Initial', when(length('Middle_Initial')==0,Null)\\\n",
    "#                        .withColumn('Last_Name', when(length('Last_Name')==0,Null)\\\n",
    "#                        .withColumn('Gender', when(length('Gender')==0,Null)\\\n",
    "#                        .withColumn(\"Date_of_Joining\", coalesce(col(\"Date_of_Joining\"), lit('00/00/000 00:00:00'))\\\n",
    "#                        .withColumn(\"Age_in_Yrs\", coalesce(col(\"Age_in_Yrs\"), lit(\"0.00\"))\\\n",
    "#                        .withColumn(\"Age_in_Company_Yrs\", coalesce(col(\"Age_in_Company_Yrs\"), lit(\"0.00\"))\\\n",
    "#                        .withColumn(\"Salary\", coalesce(col(\"Salary\"), lit(\"0.00\")))\n",
    "#                        .withColumn('Zip', F.lpad(df['Zip'], 5, '0'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir(df1_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Date_Convert =  udf (lambda x: datetime.strptime(x, '%Y%m%d'), DateType )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = data.withColumn('Emp_ID', when(length(trim(col('Emp_ID'))) == 0, trim('Emp_ID')).otherwise(0))\\\n",
    "#                        .withColumn('Date_of_Joining', when(length('Date_of_Joining')==0,'00/00/000 00:00:00')\\\n",
    "#                        .otherwise(date_format(Date_Convert(col('DATE')), 'MM-dd-yyyy'))\\ \n",
    "#                        .withColumn(\"Age_in_Yrs\", coalesce(col(\"Age_in_Yrs\"), lit(\"0.00\"))\\\n",
    "#                        .withColumn(\"Age_in_Company_Yrs\", coalesce(col(\"Age_in_Company_Yrs\"), lit(\"0.00\"))\\\n",
    "#                        .withColumn(\"Salary\", coalesce(col(\"Salary\"), lit(\"0.00\")))\\\n",
    "#                        .withColumn('Zip', F.lpad(df['Zip'], 5, '0'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df1_1.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
